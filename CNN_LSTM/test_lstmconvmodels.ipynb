{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "random.seed(43)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find device\n",
    "if torch.cuda.is_available(): # NVIDIA\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available(): # apple silicon\n",
    "    device = torch.device('mps') \n",
    "else:\n",
    "    device = torch.device('cpu') # fallback\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpus = os.cpu_count()\n",
    "print(num_cpus, 'CPUs available')\n",
    "num_cpus = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data directories\n",
    "dataai=\"../data/WISDM_ar_v1.1/\"\n",
    "\n",
    "datapath = dataai\n",
    "col_names = ['user', 'activity', 'timestamp', 'x-accel', 'y-accel', 'z-accel']\n",
    "\n",
    "df = pd.read_csv(datapath+\"WISDM_ar_v1.1_raw.txt\",\n",
    "                  header=None, names=col_names, delimiter=',', comment=';',\n",
    "                    on_bad_lines='skip') #skip/warn bad lines\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_usrs = df['user'].unique()\n",
    "print(num_usrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = df.activity.unique()\n",
    "num_channels =3 # x-accel, y-accel, z-accel\n",
    "num_classes = len(class_labels)\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_to_label = {activity: i for i, activity in enumerate(df['activity'].unique())}\n",
    "print(activity_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by users and activity\n",
    "grouped = df.groupby(['user', 'activity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X, Y, Z into a single time-series for each group\n",
    "time_series = []\n",
    "labels = []\n",
    "activity_to_label = {activity: i for i, activity in enumerate(df['activity'].unique())}\n",
    "\n",
    "for (user, activity), group in grouped:\n",
    "    # Stack X, Y, Z into a single array of shape (timesteps, 3)\n",
    "    series = np.column_stack((group['x-accel'], group['y-accel'], group['z-accel']))\n",
    "    time_series.append(series)\n",
    "    labels.append(activity_to_label[activity])\n",
    "\n",
    "print(time_series[0].shape)\n",
    "print(len(labels), np.unique(labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ynumpy = np.array(labels)\n",
    "class_weights=class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(ynumpy), y=ynumpy) \n",
    "class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "print(np.unique(ynumpy),class_weights)\n",
    "print(class_weights.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad/truncate to a fixed length\n",
    "max_length = 4*1000  # Choose a fixed length\n",
    "padded_series = nn.utils.rnn.pad_sequence([torch.tensor(series, dtype=torch.float32) for series in time_series],\n",
    "                             batch_first=True, padding_value=0)\n",
    "print(padded_series.shape)\n",
    "padded_series = padded_series[:, :max_length, :]  # Truncate to max_length if necessary\n",
    "\n",
    "\n",
    "# Normalize (not needed here, but keep it for plotting)\n",
    "# padded_series = nn.functional.normalize(padded_series, dim=2, p=2)\n",
    "print(padded_series.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized series\n",
    "num= 178\n",
    "fig, ax = plt.subplots(5, 1, figsize=(6, 8))\n",
    "i=0\n",
    "for num in [0, 50, 100, 150, 170]:\n",
    "    print(padded_series[num][99])\n",
    "    ax[i].plot(padded_series[num][:, 0],\n",
    "            label=f'X:{list(activity_to_label.keys())[list(activity_to_label.values()).index(labels[num])]}')\n",
    "    ax[i].plot(padded_series[num][:, 1],\n",
    "            label=f'Y:{list(activity_to_label.keys())[list(activity_to_label.values()).index(labels[num])]}')\n",
    "    ax[i].plot(padded_series[num][:, 2],\n",
    "            label=f'Z:{list(activity_to_label.keys())[list(activity_to_label.values()).index(labels[num])]}')\n",
    "    ax[i].legend()\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from CNN_LSTM import run_training_testing_cnnlstm\n",
    "\n",
    "# print(padded_series.shape)\n",
    "# run_training_testing_cnnlstm.run_cnnlstm_training(padded_series, labels, class_labels, \n",
    "#                                                   device, num_channels, num_classes, \n",
    "#                                                   test_size=0.45,val_size=0.45, batch_size=4, \n",
    "#                                                   num_cpus=1,lr=0.001, num_epochs=10, patience=5, \n",
    "#                                                   modeltype = \"cnnlstmskip\", max_length_series=max_length, num_conv_layers=2, \n",
    "#                                                   size_linear_lyr=10, num_blocks_per_layer=2, \n",
    "#                                                   initial_channels=8,lstm_hidden_size=16, \n",
    "#                                                   lstm_layers=1,opt=\"adamW\", verbose=False, \n",
    "#                                                   pathsave=\"./Figs\", weights=None, norm_type=\"per-channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from CNN_LSTM.nn_CNN_model import CNNI\n",
    "\n",
    "# model = CNNI(3, num_classes, max_length, num_conv_layers=3, size_linear_lyr=16)\n",
    "# # Define the input shape (input_channels, timesteps)\n",
    "# input_shape = (max_length, 3)  # max_lenth, 3 channels (X, Y, Z)\n",
    "# print(summary(model, input_size=input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from CNN_LSTM.nn_CNN_model import CNNSkipConnections\n",
    "\n",
    "# model = CNNSkipConnections(3, num_classes=num_classes, num_layers=2, num_blocks_per_layer=1, initial_channels=8)\n",
    "# # Define the input shape (input_channels, timesteps)\n",
    "# input_shape = (max_length, 3)  # max_lenth, 3 channels (X, Y, Z)\n",
    "# print(summary(model, input_size=input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN_LSTM import run_training_testing_cnnlstm\n",
    "\n",
    "# print(padded_series.shape)\n",
    "run_training_testing_cnnlstm.run_cnnlstm_training(padded_series, labels, class_labels, \n",
    "                                                  device, num_channels, num_classes, \n",
    "                                                  test_size=0.2,val_size=0.2, batch_size=4, \n",
    "                                                  num_cpus=1,lr=0.00001, num_epochs=10, patience=5, \n",
    "                                                  modeltype = \"cnnlstmskip\", max_length_series=max_length, \n",
    "                                                  num_conv_layers=4, size_linear_lyr=10, num_blocks_per_layer=2, \n",
    "                                                  initial_channels=10,lstm_hidden_size=4, \n",
    "                                                  num_lstm_layers=3,opt=\"adamW\", verbose=False, \n",
    "                                                  pathsave=\"./Figs/\", weights=None, norm_type=\"per-channel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
